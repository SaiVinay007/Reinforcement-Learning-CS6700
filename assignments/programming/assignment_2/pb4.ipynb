{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following parameterization of a policy: There is a \\preference\" for each\n",
    "action, for each column and for each row. Thus the set of preferences can be denoted by\n",
    "θx(N; 0); θy(N; 0); θx(S; 0); θy(S; 0); θx(E; 0); θy(E; 0); :::; θx(W; 9); θy(W; 9), for a total of\n",
    "80 preference values. The total preference for an action a in a state (i, j) is given by\n",
    "θx(a; i) + θy(a; j). The action probabilities are generated by a soft-max function using\n",
    "these preferences.\n",
    "4. Implement a MC policy gradient algorithm. Choose appropriate learning rates, and turn\n",
    "in two curves for each variant as indicated in the first part as well as the optimal policies\n",
    "learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_pdw\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sftmax_action(state, parameters):\n",
    "    x, y = state\n",
    "    north = parameters[x,0,0] + parameters[y,1,0]\n",
    "    east = parameters[x,0,1] + parameters[y,1,1]\n",
    "    west = parameters[x,0,2] + parameters[y,1,2]\n",
    "    south = parameters[x,0,3] + parameters[y,1,3]\n",
    "    \n",
    "    preferences = [north, east, west, south]\n",
    "    action = np.random.choice([0,1,2,3], preferences)\n",
    "    \n",
    "    return action, preferences[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_param_pos(state, action):\n",
    "    direction = action\n",
    "    row,col = state\n",
    "    return row, col, direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update(parameters, states, actions, rewards, params_pos, prob, gamma, alpha):\n",
    "    \n",
    "    # Initialize return\n",
    "    G = 0\n",
    "    \n",
    "    i=0\n",
    "    for prob, pos, reward in zip(actions[::-1], params_pos[::-1], rewards[::-1]):\n",
    "        \n",
    "        # The positions of parameters that was used to select the action in ith time step\n",
    "        row, col, direction = pos\n",
    "        \n",
    "        # The parameters that were used in action selection\n",
    "        theta1 = parameters[row, 0, direction]\n",
    "        theta2 = parameters[col, 1, direction]\n",
    "                \n",
    "        # The return \n",
    "        G = reward + pow(gamma,i)*G\n",
    "        \n",
    "        # The update equations\n",
    "        theta1 = theta1 + alpha*pow(gamma,i)*(1 - prob)\n",
    "        theta2 = theta2 + alpha*pow(gamma,i)*(1 - prob)\n",
    "        \n",
    "        parameters[row, 0, direction] = theta1\n",
    "        parameters[col, 1, direction] = theta2\n",
    "        \n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MC_PG(alpha, gamma):\n",
    "    \n",
    "    # For each row,direction and For each column,direction one parameter\n",
    "    parameters = np.ones([12,2,4])\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        # An episode\n",
    "        \n",
    "        # Store all the states, actions, rewards till the end of episode \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        # The parameters that are selected in each time step\n",
    "        params_pos = []\n",
    "        # The probability of picking those parameters\n",
    "        probs = []\n",
    "        \n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        \n",
    "        # Initialize state\n",
    "        curr_state = env.get_state()\n",
    "        \n",
    "        while True:\n",
    "            # For each step of an episode\n",
    "            \n",
    "            # select an action according to softmax probabilities\n",
    "            action, prob = sftmax_action(curr_state, parameters)\n",
    "            \n",
    "            # Get reward and next state\n",
    "            next_state, reward = env.step(action)\n",
    "            \n",
    "            probs.append(prob)\n",
    "            params_pos.append(selected_param_pos(state, action))\n",
    "            \n",
    "            states.append(curr_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Increase the number of steps\n",
    "            steps[episode] +=1\n",
    "            \n",
    "            # Average reward till now\n",
    "            avg_reward[episode] = avg_reward[episode] + (reward - avg_reward[episode])/steps[episode]\n",
    "\n",
    "            curr_state = next_state\n",
    "            print(curr_state, \"curr_state\", env.start_positions)\n",
    "\n",
    "            if curr_state == goal_pos:\n",
    "                print(\"yes\", steps[episode])\n",
    "                break\n",
    "        \n",
    "        # Update the parameters used in action selection in the episode\n",
    "        update(parameters, states, actions, rewards, params, probs, gamma, alpha)\n",
    "            \n",
    "    \n",
    "    return avg_reward, steps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    env = gym.make('gym_pdw:pdw-v0')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
